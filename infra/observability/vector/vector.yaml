data_dir: /vector-data-dir

sources:
  docker_logs:
    type: docker_logs
    auto_partial_merge: true

transforms:
  drop_rabbitmq_logs:
    type: filter
    inputs:
      - docker_logs
    condition: |
      svc = downcase(to_string(.label."com.docker.compose.service") ?? "")
      proj = downcase(to_string(.label."com.docker.compose.project") ?? "")
      cname = downcase(to_string(.container_name) ?? "")

      svc != "rabbitmq" &&
      svc != "vector-agent" &&
      svc != "signoz" &&
      svc != "otel-collector" &&
      svc != "clickhouse" &&
      svc != "zookeeper-1" &&
      svc != "schema-migrator-sync" &&
      svc != "schema-migrator-async" &&
      svc != "init-clickhouse" &&
      proj != "signoz" &&
      !starts_with(cname, "signoz-") &&
      !contains(cname, "rabbitmq") &&
      !contains(cname, "vector-agent")

  drop_s4_generate_noise:
    type: filter
    inputs:
      - drop_rabbitmq_logs
    condition: |
      svc = downcase(to_string(.label."com.docker.compose.service") ?? "")
      msg = to_string(.message) ?? ""
      !(svc == "s4-inference-engine" && match(msg, r'\[generate\] (model denoise per step|decode video frames|color correction|encode motion frames)|SoulX generated chunk'))

  normalize_docker_log:
    type: remap
    inputs:
      - drop_s4_generate_noise
    source: |
      .service = to_string(.label."com.docker.compose.service") ?? to_string(.container_name) ?? "unknown"
      .project = to_string(.label."com.docker.compose.project") ?? "unknown"
      .project_name = .project
      if .project_name == "" || .project_name == "unknown" || .project_name == "docker-compose" {
        .project_name = "${VECTOR_PROJECT_NAME:-talking-head-orchestrator-api}"
      }
      .deployment_environment = "${VECTOR_DEPLOYMENT_ENV:-local}"
      .stage = .service
      .host = get_hostname!()
      .record_id = ""
      .table_id = ""
      .event = ""

      # Keep raw string body for debugging and downstream indexing.
      .raw_message = to_string(.message) ?? ""

      # Best effort JSON parse for Loguru-JSON lines.
      parsed = parse_json(.raw_message) ?? null
      if parsed != null {
        .log_message = to_string(parsed.message) ?? .raw_message
        .level = upcase(to_string(parsed.level) ?? "INFO")
        .record_id = to_string(parsed.record_id) ?? .record_id
        .table_id = to_string(parsed.table_id) ?? .table_id
        .event = to_string(parsed.event) ?? .event
      } else {
        # Fallback for plain text Loguru lines:
        # YYYY-MM-DD HH:mm:ss.SSS | LEVEL | service | message | {extra}
        text = parse_regex(.raw_message, r'^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9:.]{12} [|][ ]*(?P<level>[A-Z]+)[ ]*[|][ ]*(?P<service>[^|]+)[|][ ]*(?P<message>.*)$') ?? null
        if text != null {
          .level = upcase(to_string(text.level))
          .log_message = strip_whitespace(to_string(text.message))

          event_match = parse_regex(.raw_message, r'event[^:]*:[^A-Za-z0-9_-]*(?P<event>[A-Za-z0-9_-]+)') ?? null
          if event_match != null {
            .event = to_string(event_match.event)
          }

          record_id_match = parse_regex(.raw_message, r'record_id[^0-9]*(?P<record_id>[0-9]+)') ?? null
          if record_id_match != null {
            .record_id = to_string(record_id_match.record_id)
          }

          table_id_match = parse_regex(.raw_message, r'table_id[^A-Za-z0-9_-]*(?P<table_id>[A-Za-z0-9_-]+)') ?? null
          if table_id_match != null {
            .table_id = to_string(table_id_match.table_id)
          }
        } else {
          .log_message = .raw_message
          .level = "INFO"
        }
      }

      .severity_text = .level
      event_time_nanos = to_unix_timestamp(.timestamp, unit: "nanoseconds") ?? to_unix_timestamp(now(), unit: "nanoseconds")

      resource_attributes = [
        { "key": "service.name", "value": { "stringValue": .service } },
        { "key": "service.namespace", "value": { "stringValue": .project_name } },
        { "key": "deployment.environment", "value": { "stringValue": .deployment_environment } },
        { "key": "host.name", "value": { "stringValue": .host } }
      ]

      log_attributes = [
        { "key": "project.name", "value": { "stringValue": .project_name } },
        { "key": "project.compose", "value": { "stringValue": .project } },
        { "key": "stage", "value": { "stringValue": .stage } },
        { "key": "record_id", "value": { "stringValue": .record_id } },
        { "key": "table_id", "value": { "stringValue": .table_id } },
        { "key": "event", "value": { "stringValue": .event } }
      ]

      # Build OTLP JSON envelope expected by Vector opentelemetry sink.
      .resourceLogs = [{
        "resource": {
          "attributes": resource_attributes
        },
        "scopeLogs": [{
          "scope": {
            "name": "vector.docker"
          },
          "logRecords": [{
            "timeUnixNano": to_string(event_time_nanos),
            "severityText": .severity_text,
            "attributes": log_attributes,
            "body": { "stringValue": .log_message }
          }]
        }]
      }]

      .message = encode_json({ "resourceLogs": .resourceLogs })
      . = { "message": .message }

sinks:
  signoz_otlp_logs:
    type: http
    inputs:
      - normalize_docker_log
    uri: ${VECTOR_SIGNOZ_OTLP_HTTP_URI:-http://host.docker.internal:4318/v1/logs}
    method: post
    encoding:
      codec: raw_message
    framing:
      method: bytes
    request:
      headers:
        content-type: application/json
    batch:
      max_events: 1
    buffer:
      type: disk
      max_size: 536870912
      when_full: block
